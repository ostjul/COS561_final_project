{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7d0b00b-5086-4870-93d0-79defe69cf33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os, sys\n",
    "\n",
    "# sys.path.insert(0, '/n/fs/ac-project/COS561/COS561_final_project/src')\n",
    "sys.path.insert(0, 'src')\n",
    "from fs_utils import remove_and_create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734fa84d-046c-4695-b5ee-10a78c3d5d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/fs/ac-project/COS561/COS561_final_project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66f9d386-2730-4830-828e-b5dff07ec5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_csvs(csv_paths: list,\n",
    "                    verbose: bool,\n",
    "                    csv_save_dir: str=None):\n",
    "\n",
    "    non_existent_paths = []\n",
    "    for csv_path in csv_paths:\n",
    "        if not os.path.exists(csv_path):\n",
    "            non_existent_paths.append(csv_path)\n",
    "\n",
    "    if len(non_existent_paths) > 0:\n",
    "        raise ValueError(\"{} paths in csv_paths do not exist: {}\".format(len(non_existent_paths), non_existent_paths))\n",
    "\n",
    "    n_csvs = len(csv_paths)\n",
    "    for csv_idx, csv_path in enumerate(csv_paths):\n",
    "        if verbose:\n",
    "            print(\"Processing {}/{} csv: {}\".format(csv_idx + 1, n_csvs, csv_path))\n",
    "\n",
    "        df = df = pd.read_csv(csv_path)\n",
    "        df = df.sort_values(['cur_hub', 'cur_port', 'etime'])\n",
    "        df['time_diff'] = df['etime'] - df['timestamp (sec)']\n",
    "        x_cols = [\n",
    "            'index', #PID?\n",
    "            'pkt len (byte)', # packet length\n",
    "            'priority',\n",
    "            'src_pc',\n",
    "            'cur_port' # in port\n",
    "        ]\n",
    "        y_col = ['time_diff']\n",
    "\n",
    "        unique_ports = df['cur_port'].unique()\n",
    "        unique_hubs = df['cur_hub'].unique()\n",
    "        \n",
    "        new_dfs = []\n",
    "        for hub in unique_hubs:\n",
    "            for port in unique_ports:\n",
    "                cur_hub_port_df = df.loc[(df['cur_port'] == port) & (df['cur_hub'] == hub)].copy()\n",
    "                len_data = len(cur_hub_port_df)\n",
    "                if verbose:\n",
    "                    print(\"hub {} port {} has {} rows\".format(hub, port, len_data))\n",
    "                # print(cur_hub_port_df['timestamp (sec)'], cur_hub_port_df['etime'])\n",
    "                # print(cur_hub_port_df)\n",
    "                loads = []\n",
    "                # Calculate the load at the current port for each row\n",
    "                for row_idx, row in cur_hub_port_df.iterrows():\n",
    "                    ingress_time = row['timestamp (sec)']\n",
    "                    egress_time = row ['etime']\n",
    "                    # load = count number of rows that have timestamp or etime between [ingress, egress]\n",
    "                    load_rows = cur_hub_port_df[\n",
    "                        # other row start in the middle of current row\n",
    "                        ((cur_hub_port_df['timestamp (sec)'] >= ingress_time) & (cur_hub_port_df['timestamp (sec)'] < egress_time)) | \n",
    "                        # other row ends in middle of current row\n",
    "                        ((cur_hub_port_df['etime'] >= ingress_time) & (cur_hub_port_df['etime'] < egress_time)) | \n",
    "                        # other row starts before current row and ends after current row\n",
    "                        ((cur_hub_port_df['timestamp (sec)'] < ingress_time) & (cur_hub_port_df['etime'] > egress_time))]\n",
    "                    # Count number of rows \n",
    "                    load = len(load_rows)\n",
    "                    # if load > 10:\n",
    "                    #     print(\"ingress time {} egress time {}\".format(ingress_time, egress_time))\n",
    "                    #     print(load_rows['timestamp (sec)'], load_rows['etime'])\n",
    "                    loads.append(load)\n",
    "                # Assign load column and append to list of dataframe \n",
    "                cur_hub_port_df['load'] = loads\n",
    "                new_dfs.append(cur_hub_port_df)\n",
    "        # Concatenate data frames for each device/port combination\n",
    "        new_df = pd.concat(new_dfs)\n",
    "        \n",
    "        # Calculate average load for each port\n",
    "        for port in unique_ports:\n",
    "            avg_load = np.mean(new_df[new_df['cur_port'] == port]['load'].to_numpy())\n",
    "            new_df['mean_load_port_{}'.format(port)] = avg_load\n",
    "            if verbose:\n",
    "                print(\"average load for port {}: {}\".format(port, avg_load))\n",
    "        \n",
    "        # Save new CSV\n",
    "        csv_save_name = os.path.splitext(os.path.basename(csv_path))[0] + '_processed.csv'\n",
    "        csv_save_path = os.path.join(csv_save_dir, csv_save_name)\n",
    "        \n",
    "        new_df.to_csv(csv_save_path)\n",
    "        print(\"Saved processed csv to {}\\n\".format(csv_save_path))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b341f34-84b4-4eb9-b7b5-613682942b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/1 csv: data/rsim1_dummy.csv\n",
      "hub 12 port 0 has 18 rows\n",
      "hub 12 port 1 has 28 rows\n",
      "hub 13 port 0 has 12 rows\n",
      "hub 13 port 1 has 20 rows\n",
      "hub 14 port 0 has 17 rows\n",
      "hub 14 port 1 has 31 rows\n",
      "hub 15 port 0 has 23 rows\n",
      "hub 15 port 1 has 24 rows\n",
      "hub 16 port 0 has 18 rows\n",
      "hub 16 port 1 has 28 rows\n",
      "hub 17 port 0 has 32 rows\n",
      "hub 17 port 1 has 27 rows\n",
      "hub 18 port 0 has 17 rows\n",
      "hub 18 port 1 has 36 rows\n",
      "hub 19 port 0 has 33 rows\n",
      "hub 19 port 1 has 35 rows\n",
      "average load for port 0: 8.329411764705883\n",
      "average load for port 1: 10.240174672489083\n",
      "Saved processed csv to data/processed_data/rsim1_dummy_processed.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_root_dir =  os.path.join('data', 'dqn_data') # TODO: replace with your root if necessary\n",
    "csv_path = os.path.join('data', 'rsim1.csv')\n",
    "csv_dummy_path = os.path.join(os.path.dirname(csv_path), 'rsim1_dummy.csv')\n",
    "\n",
    "csv_save_dir = os.path.join('data', 'processed_data')\n",
    "remove_and_create_dir(csv_save_dir)\n",
    "preprocess_csvs(\n",
    "    [csv_dummy_path], \n",
    "    verbose=True,\n",
    "    csv_save_dir=csv_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2ec06ac-49af-44da-98b7-cc8031520372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class TracesDataset(Dataset):\n",
    "#     def __init__(self,\n",
    "#                  csv_paths: list,\n",
    "#                  n_timesteps: int,\n",
    "#                  y_label: str):\n",
    "\n",
    "#         self.n_timesteps = n_timesteps\n",
    "#         self.indices = [] # Tuples of (csv_idx, device_idx, row_idx)\n",
    "#         self.xs = []\n",
    "#         self.ys = []\n",
    "#         for csv_idx, csv_path in enumerate(csv_paths):\n",
    "#             # load CSV, create empty list\n",
    "#             # separate by device and store each df as numpy array in a list\n",
    "#             # for each device, get range of valid start idxs to have complete timeseries data\n",
    "#             # make into tuples with csv_idx and device_idx and append to self.indices\n",
    "#             df = pd.read_csv(csv_path)\n",
    "#             unique_devices = df['cur_hub'].unique()\n",
    "#             xs_csv = []\n",
    "#             ys_csv = []\n",
    "#             for device_idx, unique_device in enumerate(unique_devices):\n",
    "#                 device_data = df.loc[df['cur_hub'] == unique_device]\n",
    "#                 print(device_data.columns)\n",
    "#                 # Separate x and y values for this device\n",
    "#                 xs_device = device_data.drop(y_label, axis=1).to_numpy()\n",
    "#                 ys_device = device_data[y_label].to_numpy()\n",
    "                \n",
    "#                 # Append to list of data for each csv\n",
    "#                 xs_csv.append(xs_device)\n",
    "#                 ys_csv.append(ys_device)\n",
    "                \n",
    "#                 # Calculate indices for timeseries\n",
    "#                 n_rows = device_data.shape[0]\n",
    "#                 n_timeseries_data = n_rows - self.n_timesteps + 1\n",
    "#                 # Create tuples using CSV index, device index, and rows\n",
    "#                 timeseries_idxs = [(csv_idx, device_idx, row_idx) for row_idx in range(n_timeseries_data)]\n",
    "#                 self.indices += timeseries_idxs\n",
    "\n",
    "#             self.xs.append(xs_csv)\n",
    "#             self.ys.append(ys_csv)\n",
    "            \n",
    "#     def __getitem__(self, index):\n",
    "#         csv_idx, device_idx, row_start_idx = self.indices[index]\n",
    "#         xs = self.xs[csv_idx][device_idx][row_start_idx:row_start_idx + self.n_timesteps]\n",
    "#         ys = self.ys[csv_idx][device_idx][row_start_idx:row_start_idx + self.n_timesteps]\n",
    "#         return xs, ys\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d3c70b0-4ad2-46a1-93d5-0c9f24cb19c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "Index(['index', 'timestamp (sec)', 'pkt len (byte)', 'priority', 'src_pc',\n",
      "       'cur_hub', 'cur_port', 'path', 'etime'],\n",
      "      dtype='object')\n",
      "288080\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n",
      "(15, 8) (15,)\n"
     ]
    }
   ],
   "source": [
    "data_root_dir =  os.path.join('data', 'dqn_data') # TODO: replace with your root if necessary\n",
    "csv_path = os.path.join('data', 'rsim1.csv')\n",
    "\n",
    "csv_dummy_path = os.path.join(os.path.dirname(csv_path), 'rsim1_dummy.csv')\n",
    "dataset = TracesDataset(\n",
    "    csv_paths=[csv_path],  # can change this to csv_path, but it takes a very long time to load!\n",
    "    n_timesteps=15,\n",
    "    y_label='etime')\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "for idx, (x, y) in enumerate(dataset):\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cf45e-fcff-471f-a49b-95162ad0aad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
