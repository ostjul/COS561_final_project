{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b571a00-4568-448d-b5e8-e6ebb1054eaf",
   "metadata": {},
   "source": [
    "## Preprocessing Data Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9c9cca4-fd94-40db-af06-278ed6cca196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Manager\n",
    "import numba\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from fs_utils import remove_and_create_dir\n",
    "from dqn_src.config import BaseConfig\n",
    "from dqn_src.traffic_indicator import TrafficIndicatorFeature\n",
    "from dqn_src import builtseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e3adac7-8b2a-4b87-adad-6e7d3b38b6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = BaseConfig()\n",
    "target = ['time_in_sys']\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# CHANGE THIS IF NEEDED\n",
    "data_root = os.path.join('/scratch', 'achen', 'COS561', 'deepqueuenet','data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c57df3f-65ee-4233-a09f-b8e61d9ea32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_process(func, FILES, args=None):\n",
    "    print(\"in multi_process()\")\n",
    "    print(\"Length of FILES: {}\".format(len(FILES)))\n",
    "    it = 0\n",
    "    while True:\n",
    "        files = FILES[it * config.no_process:(it + 1) *\n",
    "                      config.no_process]\n",
    "        print(\"[{}] Itr {} processing {} files\".format(datetime.now().strftime(r'%m%d_%H%M%S'), it+1, len(files)))\n",
    "        if len(files) > 0:\n",
    "            threads = []\n",
    "            for file in files:\n",
    "                ARGS = list(args)\n",
    "                ARGS.append(file)\n",
    "                t = Process(target=func, args=tuple(ARGS))\n",
    "                threads.append(t)\n",
    "                t.start()\n",
    "            for thr in threads:\n",
    "                thr.join()\n",
    "            it += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fceaefe0-0ff7-4fe5-b433-857b10f30385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def feature_extraction(config: BaseConfig,\n",
    "                       target: list[str],\n",
    "                       data_root: str):\n",
    "    \"\"\"feature extraction module\"\"\"\n",
    "\n",
    "    # @numba.jit\n",
    "    def gettraffic(dst_folder, my_fet, file):\n",
    "        df = pd.read_csv(file).fillna(config.sp_wgt)\n",
    "\n",
    "        #traffic load features\n",
    "        ins = TrafficIndicatorFeature(df, config.no_of_port, config.no_of_buffer,\n",
    "                      config.window, config.ser_rate)\n",
    "        C_dst_SET, LOAD = ins.getCount()\n",
    "        for i in range(config.no_of_port):\n",
    "            df['TI%i' % i] = LOAD[i]\n",
    "        for i in range(config.no_of_port):\n",
    "            for j in range(config.no_of_buffer):\n",
    "                df['load_dst{}_{}'.format(i, j)] = C_dst_SET[(i, j)]\n",
    "\n",
    "        #arrival patterns\n",
    "        df['inter_arr_sys'] = df['timestamp (sec)'].diff()\n",
    "        if config.no_of_buffer > 1:\n",
    "            for i in range(config.no_of_buffer):\n",
    "                t = df[df['priority'] ==\n",
    "                       i]['timestamp (sec)'].diff().rename(\n",
    "                           'inter_arr{}'.format(i)).to_frame()\n",
    "                df = df.join(t)\n",
    "\n",
    "        #save\n",
    "        filename = file.split('/')[-1]\n",
    "        drop_cols = ['timestamp (sec)'] + target\n",
    "        if config.no_of_buffer == 1: drop_cols += ['priority']\n",
    "        fet_cols = list(df.columns.drop(drop_cols))\n",
    "        my_fet['fet_cols'] = fet_cols\n",
    "        df[fet_cols + target].fillna(method='ffill').dropna().to_csv(\n",
    "            '{}/{}'.format(dst_folder, filename), index=False)\n",
    "\n",
    "    with Manager() as MG:\n",
    "        my_fet = MG.dict()\n",
    "        for mode in ['train', 'test']:\n",
    "            file_dir = os.path.join(data_root, '/{}/_traces/_{}'.format(\n",
    "                config.modelname, mode))\n",
    "            FILES = []\n",
    "            for dirpath, dirnames, filenames in os.walk(file_dir):\n",
    "                for file in filenames:\n",
    "                    if (os.path.splitext(file)[1]\n",
    "                            == '.csv') and 'checkpoint' not in file:\n",
    "                        FILES.append(os.path.join(dirpath, file))\n",
    "            \n",
    "            dst_folder = os.path.join(data_root, '{}/_traces/{}'.format(\n",
    "                config.modelname, mode))\n",
    "            if os.path.exists(dst_folder):\n",
    "                shutil.rmtree(dst_folder)\n",
    "            os.makedirs(dst_folder)\n",
    "            multi_process(gettraffic,\n",
    "                               FILES,\n",
    "                               args=(dst_folder, my_fet))\n",
    "\n",
    "        fet_cols = my_fet['fet_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43b26ae6-726a-4898-8f12-4282788e3675",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features already exist at /scratch/achen/COS561/deepqueuenet/data/4-port switch/FIFO/_traces/train and /scratch/achen/COS561/deepqueuenet/data/4-port switch/FIFO/_traces/test\n"
     ]
    }
   ],
   "source": [
    "train_features_path = os.path.join(data_root, config.modelname, '_traces', 'train')\n",
    "test_features_path = os.path.join(data_root, config.modelname, '_traces', 'test')\n",
    "if os.path.exists(train_features_path) and os.path.exists(test_features_path):\n",
    "    print(\"Extracted features already exist at {} and {}\".format(train_features_path, test_features_path))\n",
    "else:\n",
    "    feature_extraction(\n",
    "        config=config,\n",
    "        target=target,\n",
    "        data_root=data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058ecbb-506d-4cc9-916b-87563ea6a098",
   "metadata": {},
   "source": [
    "### Convert CSV -> .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b91959f4-3e05-43e0-bab8-138e7b8216ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_hdf(file):\n",
    "    with h5py.File(file, 'r') as hdf:\n",
    "        x = hdf['x'][:]\n",
    "        y = hdf['y'][:]\n",
    "    return x, y\n",
    "\n",
    "def write_hdf(file, x, y):\n",
    "    with h5py.File(file, 'w') as hdf:\n",
    "        hdf['x'] = x\n",
    "        hdf['y'] = y\n",
    "\n",
    "def write_hdf2(h, key, x, y):\n",
    "    h['{}_x'.format(key)] = x\n",
    "    h['{}_y'.format(key)] = y\n",
    "\n",
    "def csv_2hdf(data_root: str,\n",
    "             config: BaseConfig):\n",
    "    \"\"\"\n",
    "    Build timeseries batches for bLSTM and save them in .hdf files\n",
    "      - split train mode files for train and in-sample testing (test1);\n",
    "      - save test mode files for out-of-sample testing (test2).\n",
    "    \"\"\"\n",
    "\n",
    "    # @numba.jit\n",
    "    def split_2hdf(mode, file):\n",
    "        key = file.split('/')[-1].split('.csv')[0]\n",
    "        t = pd.read_csv(file)\n",
    "        # os.remove(file)\n",
    "\n",
    "        ins = builtseq.build_timeseries(t.values, target_col_index=[-1])\n",
    "        x, y = ins.timeseries(config.TIME_STEPS)\n",
    "        \"randomly selected part of the them. to represent the config\"\n",
    "        loc = np.random.choice(\n",
    "            np.arange(len(y)),\n",
    "            np.max([np.min([14000, len(y)]),\n",
    "                    int(len(y) * 0.15)]),\n",
    "            replace=False)\n",
    "        x = x[loc]\n",
    "        y = y[loc]\n",
    "        if mode == 'train':\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                x, y, test_size=config.test_size, shuffle=True)\n",
    "            write_hdf(\n",
    "                os.path.join(data_root, '{}/_hdf/train/{}.h5'.format(\n",
    "                    config.modelname, key)), x_train, y_train)\n",
    "            write_hdf(\n",
    "                os.path.join(data_root, '{}/_hdf/test1/{}.h5'.format(\n",
    "                    config.modelname, key)), x_test, y_test)\n",
    "        else:\n",
    "            write_hdf(\n",
    "                os.path.join(data_root, '{}/_hdf/test2/{}.h5'.format(\n",
    "                    config.modelname, key)), x, y)\n",
    "\n",
    "    for mode in ['train', 'test']:\n",
    "        folder = os.path.join(data_root, '{}/_traces/{}'.format(config.modelname, mode))\n",
    "        # Create directories for each split\n",
    "        hdf_folder = os.path.join(data_root, config.modelname, '_hdf')\n",
    "        splits = ['train', 'test1', 'test2']\n",
    "        for split in splits:\n",
    "            split_folder = os.path.join(hdf_folder, split)\n",
    "            if os.path.exists(split_folder):\n",
    "                shutil.rmtree(split_folder)\n",
    "            os.makedirs(split_folder)\n",
    "            print(\"Created directory for {}\".format(split_folder))\n",
    "        print(\"FILES folder: {}\".format(folder))\n",
    "        FILES = glob.glob('{}/*.csv'.format(folder))\n",
    "        multi_process(split_2hdf, FILES, args=(mode, ))\n",
    "        # shutil.rmtree(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d5e40b7-ca27-48d7-8d1e-d4d1b4d7f5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HD5 already exist at /scratch/achen/COS561/deepqueuenet/data/4-port switch/FIFO/_traces/train, /scratch/achen/COS561/deepqueuenet/data/4-port switch/FIFO/_hdf/test1, and /scratch/achen/COS561/deepqueuenet/data/4-port switch/FIFO/_hdf/test2\n"
     ]
    }
   ],
   "source": [
    "train_hdf_path = os.path.join(data_root, config.modelname, '_hdf', 'train')\n",
    "test1_hdf_path = os.path.join(data_root, config.modelname, '_hdf', 'test1')\n",
    "test2_hdf_path = os.path.join(data_root, config.modelname, '_hdf', 'test2')\n",
    "if os.path.exists(train_features_path) and os.path.exists(test1_hdf_path) and os.path.exists(test2_hdf_path):\n",
    "    print(\"HD5 already exist at {}, {}, and {}\".format(train_features_path, test1_hdf_path, test2_hdf_path))\n",
    "else:\n",
    "    csv_2hdf(\n",
    "        config=config,\n",
    "        data_root=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749db673-b661-4a3d-988a-78f4c3d3b06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
