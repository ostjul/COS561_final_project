{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b571a00-4568-448d-b5e8-e6ebb1054eaf",
   "metadata": {},
   "source": [
    "## Preprocessing Data Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c9cca4-fd94-40db-af06-278ed6cca196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from multiprocessing import Process, Manager\n",
    "import numba\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from fs_utils import remove_and_create_dir\n",
    "from dqn_src.config import BaseConfig\n",
    "from dqn_src.traffic_indicator import TrafficIndicatorFeature\n",
    "from dqn_src import builtseq\n",
    "from dqn_src.min_max_scaler import cScaler, load_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3adac7-8b2a-4b87-adad-6e7d3b38b6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = BaseConfig()\n",
    "target = ['time_in_sys']\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# CHANGE THIS IF NEEDED\n",
    "data_root = os.path.join('/scratch/achen/COS561/deepqueuenet/dqn_data/data/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad270a3-6435-46aa-8904-144a7a02c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildfolder(data_root, model_name):\n",
    "    for name in [\n",
    "            '_hdf', '_hdf/train', '_hdf/test1', '_hdf/test2', '_scaler',\n",
    "            '_tfrecords', '_error'\n",
    "    ]:\n",
    "        folder = os.path.join(data_root, 'data/{}/{}'.format(model_name, name))\n",
    "        if os.path.exists(folder):\n",
    "            # shutil.rmtree(folder)\n",
    "            continue\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "buildfolder(\n",
    "    data_root=data_root,\n",
    "    model_name=config.modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c57df3f-65ee-4233-a09f-b8e61d9ea32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_process(func, FILES, args=None):\n",
    "    print(\"in multi_process()\")\n",
    "    print(\"Length of FILES: {}\".format(len(FILES)))\n",
    "    it = 0\n",
    "    while True:\n",
    "        files = FILES[it * config.no_process:(it + 1) *\n",
    "                      config.no_process]\n",
    "        print(\"[{}] Itr {} processing {} files\".format(datetime.now().strftime(r'%m%d_%H%M%S'), it+1, len(files)))\n",
    "        if len(files) > 0:\n",
    "            threads = []\n",
    "            for file in files:\n",
    "                ARGS = list(args)\n",
    "                ARGS.append(file)\n",
    "                t = Process(target=func, args=tuple(ARGS))\n",
    "                threads.append(t)\n",
    "                t.start()\n",
    "            for thr in threads:\n",
    "                thr.join()\n",
    "            it += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fceaefe0-0ff7-4fe5-b433-857b10f30385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def feature_extraction(config: BaseConfig,\n",
    "                       target: list[str],\n",
    "                       data_root: str):\n",
    "    \n",
    "    \"\"\"feature extraction module\"\"\"\n",
    "\n",
    "    # @numba.jit\n",
    "    def gettraffic(dst_folder, my_fet, file):\n",
    "        df = pd.read_csv(file).fillna(config.sp_wgt)\n",
    "\n",
    "        #traffic load features\n",
    "        ins = TrafficIndicatorFeature(df, config.no_of_port, config.no_of_buffer,\n",
    "                      config.window, config.ser_rate)\n",
    "        C_dst_SET, LOAD = ins.getCount()\n",
    "        for i in range(config.no_of_port):\n",
    "            df['TI%i' % i] = LOAD[i]\n",
    "        for i in range(config.no_of_port):\n",
    "            for j in range(config.no_of_buffer):\n",
    "                df['load_dst{}_{}'.format(i, j)] = C_dst_SET[(i, j)]\n",
    "\n",
    "        #arrival patterns\n",
    "        df['inter_arr_sys'] = df['timestamp (sec)'].diff()\n",
    "        if config.no_of_buffer > 1:\n",
    "            for i in range(config.no_of_buffer):\n",
    "                t = df[df['priority'] ==\n",
    "                       i]['timestamp (sec)'].diff().rename(\n",
    "                           'inter_arr{}'.format(i)).to_frame()\n",
    "                df = df.join(t)\n",
    "\n",
    "        #save\n",
    "        filename = file.split('/')[-1]\n",
    "        drop_cols = ['timestamp (sec)'] + target\n",
    "        if config.no_of_buffer == 1: drop_cols += ['priority']\n",
    "        fet_cols = list(df.columns.drop(drop_cols))\n",
    "        my_fet['fet_cols'] = fet_cols\n",
    "        df[fet_cols + target].fillna(method='ffill').dropna().to_csv(\n",
    "            '{}/{}'.format(dst_folder, filename), index=False)\n",
    "\n",
    "    with Manager() as MG:\n",
    "        my_fet = MG.dict()\n",
    "        for mode in ['train', 'test']:\n",
    "            print(data_root)\n",
    "            file_dir = os.path.join(data_root, '{}/_traces/_{}'.format(config.modelname, mode))\n",
    "            print(data_root)\n",
    "            print(\"Looking in {} for files.\".format(file_dir))\n",
    "            FILES = []\n",
    "            for dirpath, dirnames, filenames in os.walk(file_dir):\n",
    "                for file in filenames:\n",
    "                    if (os.path.splitext(file)[1]\n",
    "                            == '.csv') and 'checkpoint' not in file:\n",
    "                        FILES.append(os.path.join(dirpath, file))\n",
    "            \n",
    "            dst_folder = os.path.join(data_root, '{}/_traces/{}'.format(\n",
    "                config.modelname, mode))\n",
    "            if os.path.exists(dst_folder):\n",
    "                shutil.rmtree(dst_folder)\n",
    "            os.makedirs(dst_folder)\n",
    "            multi_process(gettraffic,\n",
    "                               FILES,\n",
    "                               args=(dst_folder, my_fet))\n",
    "\n",
    "            \n",
    "        fet_cols = my_fet['fet_cols']\n",
    "        \n",
    "    feature_columns_save_path = os.path.join(data_root, config.modelname, 'feature_columns.pt')\n",
    "    torch.save(fet_cols, feature_columns_save_path)\n",
    "    return fet_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b26ae6-726a-4898-8f12-4282788e3675",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/achen/COS561/deepqueuenet/dqn_data/data/\n",
      "/scratch/achen/COS561/deepqueuenet/dqn_data/data/\n",
      "Looking in /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_traces/_train for files.\n",
      "in multi_process()\n",
      "Length of FILES: 1260\n",
      "[0321_180027] Itr 1 processing 15 files\n",
      "[0321_180047] Itr 2 processing 15 files\n",
      "[0321_180111] Itr 3 processing 15 files\n",
      "[0321_180138] Itr 4 processing 15 files\n",
      "[0321_180205] Itr 5 processing 15 files\n",
      "[0321_180229] Itr 6 processing 15 files\n",
      "[0321_180250] Itr 7 processing 15 files\n",
      "[0321_180314] Itr 8 processing 15 files\n",
      "[0321_180335] Itr 9 processing 15 files\n",
      "[0321_180401] Itr 10 processing 15 files\n",
      "[0321_180423] Itr 11 processing 15 files\n",
      "[0321_180446] Itr 12 processing 15 files\n",
      "[0321_180512] Itr 13 processing 15 files\n",
      "[0321_180537] Itr 14 processing 15 files\n",
      "[0321_180604] Itr 15 processing 15 files\n",
      "[0321_180624] Itr 16 processing 15 files\n",
      "[0321_180652] Itr 17 processing 15 files\n",
      "[0321_180716] Itr 18 processing 15 files\n",
      "[0321_180735] Itr 19 processing 15 files\n",
      "[0321_180757] Itr 20 processing 15 files\n",
      "[0321_180821] Itr 21 processing 15 files\n",
      "[0321_180845] Itr 22 processing 15 files\n",
      "[0321_180911] Itr 23 processing 15 files\n",
      "[0321_180936] Itr 24 processing 15 files\n",
      "[0321_180958] Itr 25 processing 15 files\n",
      "[0321_181021] Itr 26 processing 15 files\n",
      "[0321_181043] Itr 27 processing 15 files\n",
      "[0321_181110] Itr 28 processing 15 files\n",
      "[0321_181137] Itr 29 processing 15 files\n",
      "[0321_181157] Itr 30 processing 15 files\n",
      "[0321_181221] Itr 31 processing 15 files\n",
      "[0321_181243] Itr 32 processing 15 files\n",
      "[0321_181309] Itr 33 processing 15 files\n",
      "[0321_181332] Itr 34 processing 15 files\n",
      "[0321_181357] Itr 35 processing 15 files\n",
      "[0321_181417] Itr 36 processing 15 files\n",
      "[0321_181444] Itr 37 processing 15 files\n",
      "[0321_181509] Itr 38 processing 15 files\n",
      "[0321_181533] Itr 39 processing 15 files\n",
      "[0321_181556] Itr 40 processing 15 files\n",
      "[0321_181620] Itr 41 processing 15 files\n",
      "[0321_181646] Itr 42 processing 15 files\n",
      "[0321_181709] Itr 43 processing 15 files\n",
      "[0321_181732] Itr 44 processing 15 files\n",
      "[0321_181756] Itr 45 processing 15 files\n",
      "[0321_181820] Itr 46 processing 15 files\n",
      "[0321_181843] Itr 47 processing 15 files\n",
      "[0321_181908] Itr 48 processing 15 files\n",
      "[0321_181931] Itr 49 processing 15 files\n",
      "[0321_181952] Itr 50 processing 15 files\n",
      "[0321_182014] Itr 51 processing 15 files\n",
      "[0321_182042] Itr 52 processing 15 files\n",
      "[0321_182104] Itr 53 processing 15 files\n",
      "[0321_182134] Itr 54 processing 15 files\n",
      "[0321_182153] Itr 55 processing 15 files\n",
      "[0321_182216] Itr 56 processing 15 files\n",
      "[0321_182239] Itr 57 processing 15 files\n",
      "[0321_182304] Itr 58 processing 15 files\n",
      "[0321_182328] Itr 59 processing 15 files\n",
      "[0321_182353] Itr 60 processing 15 files\n",
      "[0321_182417] Itr 61 processing 15 files\n",
      "[0321_182441] Itr 62 processing 15 files\n",
      "[0321_182503] Itr 63 processing 15 files\n",
      "[0321_182528] Itr 64 processing 15 files\n",
      "[0321_182549] Itr 65 processing 15 files\n",
      "[0321_182614] Itr 66 processing 15 files\n",
      "[0321_182639] Itr 67 processing 15 files\n",
      "[0321_182701] Itr 68 processing 15 files\n",
      "[0321_182726] Itr 69 processing 15 files\n",
      "[0321_182750] Itr 70 processing 15 files\n",
      "[0321_182816] Itr 71 processing 15 files\n",
      "[0321_182839] Itr 72 processing 15 files\n",
      "[0321_182902] Itr 73 processing 15 files\n",
      "[0321_182930] Itr 74 processing 15 files\n",
      "[0321_182954] Itr 75 processing 15 files\n",
      "[0321_183015] Itr 76 processing 15 files\n",
      "[0321_183043] Itr 77 processing 15 files\n"
     ]
    }
   ],
   "source": [
    "train_features_path = os.path.join(data_root, config.modelname, '_traces', 'train')\n",
    "test_features_path = os.path.join(data_root, config.modelname, '_traces', 'test')\n",
    "if os.path.exists(train_features_path) and os.path.exists(test_features_path):\n",
    "    print(\"Extracted features already exist at {} and {}\".format(train_features_path, test_features_path))\n",
    "\n",
    "feature_columns = feature_extraction(\n",
    "    config=config,\n",
    "    target=target,\n",
    "    data_root=data_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f937ef3-4bdb-4995-9bb0-feacc09fe8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = ['pkt len (byte)', 'src', 'dst', 'TI0', 'TI1', 'TI2', 'TI3', 'load_dst0_0', 'load_dst1_0', 'load_dst2_0', 'load_dst3_0', 'inter_arr_sys']\n",
    "feature_columns_save_path = os.path.join(data_root, config.modelname, 'feature_columns.pt')\n",
    "if not os.path.exists(feature_columns_save_path):\n",
    "    torch.save(feature_columns, feature_columns_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058ecbb-506d-4cc9-916b-87563ea6a098",
   "metadata": {},
   "source": [
    "### Convert CSV -> .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91959f4-3e05-43e0-bab8-138e7b8216ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_hdf(file):\n",
    "    with h5py.File(file, 'r') as hdf:\n",
    "        x = hdf['x'][:]\n",
    "        y = hdf['y'][:]\n",
    "    return x, y\n",
    "\n",
    "def write_hdf(file, x, y):\n",
    "    with h5py.File(file, 'w') as hdf:\n",
    "        hdf['x'] = x\n",
    "        hdf['y'] = y\n",
    "\n",
    "def write_hdf2(h, key, x, y):\n",
    "    h['{}_x'.format(key)] = x\n",
    "    h['{}_y'.format(key)] = y\n",
    "\n",
    "def csv_2hdf(data_root: str,\n",
    "             config: BaseConfig):\n",
    "    \"\"\"\n",
    "    Build timeseries batches for bLSTM and save them in .hdf files\n",
    "      - split train mode files for train and in-sample testing (test1);\n",
    "      - save test mode files for out-of-sample testing (test2).\n",
    "    \"\"\"\n",
    "\n",
    "    # @numba.jit\n",
    "    def split_2hdf(mode, file):\n",
    "        key = file.split('/')[-1].split('.csv')[0]\n",
    "        t = pd.read_csv(file)\n",
    "        # os.remove(file)\n",
    "\n",
    "        ins = builtseq.build_timeseries(t.values, target_col_index=[-1])\n",
    "        x, y = ins.timeseries(config.TIME_STEPS)\n",
    "        \"randomly selected part of the them. to represent the config\"\n",
    "        loc = np.random.choice(\n",
    "            np.arange(len(y)),\n",
    "            np.max([np.min([14000, len(y)]),\n",
    "                    int(len(y) * 0.15)]),\n",
    "            replace=False)\n",
    "        x = x[loc]\n",
    "        y = y[loc]\n",
    "        if mode == 'train':\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                x, y, test_size=config.test_size, shuffle=True)\n",
    "            write_hdf(\n",
    "                os.path.join(data_root, '{}/_hdf/train/{}.h5'.format(\n",
    "                    config.modelname, key)), x_train, y_train)\n",
    "            write_hdf(\n",
    "                os.path.join(data_root, '{}/_hdf/test1/{}.h5'.format(\n",
    "                    config.modelname, key)), x_test, y_test)\n",
    "        else:\n",
    "            write_hdf(\n",
    "                os.path.join(data_root, '{}/_hdf/test2/{}.h5'.format(\n",
    "                    config.modelname, key)), x, y)\n",
    "    \n",
    "    # for mode in ['train', 'test']:\n",
    "    #         folder = os.path.join(data_root, '{}/_traces/{}'.format(config.modelname, mode))\n",
    "    #         FILES = glob.glob('{}/*.csv'.format(folder))\n",
    "    #         multi_process(split_2hdf, FILES, args=(mode, ))\n",
    "    #         # shutil.rmtree(folder)\n",
    "    \n",
    "    # Create directories for each split\n",
    "    hdf_folder = os.path.join(data_root, config.modelname, '_hdf')\n",
    "    splits = ['train', 'test1', 'test2']\n",
    "    for split in splits:\n",
    "        split_folder = os.path.join(hdf_folder, split)\n",
    "        if os.path.exists(split_folder):\n",
    "            shutil.rmtree(split_folder)\n",
    "        os.makedirs(split_folder)\n",
    "        print(\"Created directory for {}\".format(split_folder))\n",
    "        \n",
    "    for mode in ['train', 'test']:\n",
    "        folder = os.path.join(data_root, '{}/_traces/{}'.format(config.modelname, mode))\n",
    "        \n",
    "        \n",
    "        print(\"FILES folder: {}\".format(folder))\n",
    "        \n",
    "        FILES = glob.glob('{}/*.csv'.format(folder))\n",
    "        multi_process(split_2hdf, FILES, args=(mode, ))\n",
    "        shutil.rmtree(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d5e40b7-ca27-48d7-8d1e-d4d1b4d7f5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory for /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_hdf/train\n",
      "Created directory for /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_hdf/test1\n",
      "Created directory for /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_hdf/test2\n",
      "FILES folder: /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_traces/train\n",
      "in multi_process()\n",
      "Length of FILES: 1260\n",
      "[0321_201045] Itr 1 processing 15 files\n",
      "[0321_201047] Itr 2 processing 15 files\n",
      "[0321_201049] Itr 3 processing 15 files\n",
      "[0321_201050] Itr 4 processing 15 files\n",
      "[0321_201052] Itr 5 processing 15 files\n",
      "[0321_201054] Itr 6 processing 15 files\n",
      "[0321_201055] Itr 7 processing 15 files\n",
      "[0321_201057] Itr 8 processing 15 files\n",
      "[0321_201058] Itr 9 processing 15 files\n",
      "[0321_201100] Itr 10 processing 15 files\n",
      "[0321_201101] Itr 11 processing 15 files\n",
      "[0321_201103] Itr 12 processing 15 files\n",
      "[0321_201104] Itr 13 processing 15 files\n",
      "[0321_201106] Itr 14 processing 15 files\n",
      "[0321_201108] Itr 15 processing 15 files\n",
      "[0321_201110] Itr 16 processing 15 files\n",
      "[0321_201112] Itr 17 processing 15 files\n",
      "[0321_201114] Itr 18 processing 15 files\n",
      "[0321_201117] Itr 19 processing 15 files\n",
      "[0321_201119] Itr 20 processing 15 files\n",
      "[0321_201122] Itr 21 processing 15 files\n",
      "[0321_201124] Itr 22 processing 15 files\n",
      "[0321_201126] Itr 23 processing 15 files\n",
      "[0321_201129] Itr 24 processing 15 files\n",
      "[0321_201134] Itr 25 processing 15 files\n",
      "[0321_201136] Itr 26 processing 15 files\n",
      "[0321_201138] Itr 27 processing 15 files\n",
      "[0321_201141] Itr 28 processing 15 files\n",
      "[0321_201143] Itr 29 processing 15 files\n",
      "[0321_201145] Itr 30 processing 15 files\n",
      "[0321_201154] Itr 31 processing 15 files\n",
      "[0321_201203] Itr 32 processing 15 files\n",
      "[0321_201209] Itr 33 processing 15 files\n",
      "[0321_201211] Itr 34 processing 15 files\n",
      "[0321_201214] Itr 35 processing 15 files\n",
      "[0321_201219] Itr 36 processing 15 files\n",
      "[0321_201221] Itr 37 processing 15 files\n",
      "[0321_201224] Itr 38 processing 15 files\n",
      "[0321_201226] Itr 39 processing 15 files\n",
      "[0321_201228] Itr 40 processing 15 files\n",
      "[0321_201231] Itr 41 processing 15 files\n",
      "[0321_201233] Itr 42 processing 15 files\n",
      "[0321_201235] Itr 43 processing 15 files\n",
      "[0321_201238] Itr 44 processing 15 files\n",
      "[0321_201240] Itr 45 processing 15 files\n",
      "[0321_201242] Itr 46 processing 15 files\n",
      "[0321_201247] Itr 47 processing 15 files\n",
      "[0321_201249] Itr 48 processing 15 files\n",
      "[0321_201251] Itr 49 processing 15 files\n",
      "[0321_201254] Itr 50 processing 15 files\n",
      "[0321_201258] Itr 51 processing 15 files\n",
      "[0321_201301] Itr 52 processing 15 files\n",
      "[0321_201302] Itr 53 processing 15 files\n",
      "[0321_201305] Itr 54 processing 15 files\n",
      "[0321_201308] Itr 55 processing 15 files\n",
      "[0321_201316] Itr 56 processing 15 files\n",
      "[0321_201318] Itr 57 processing 15 files\n",
      "[0321_201320] Itr 58 processing 15 files\n",
      "[0321_201323] Itr 59 processing 15 files\n",
      "[0321_201325] Itr 60 processing 15 files\n",
      "[0321_201331] Itr 61 processing 15 files\n",
      "[0321_201333] Itr 62 processing 15 files\n",
      "[0321_201335] Itr 63 processing 15 files\n",
      "[0321_201337] Itr 64 processing 15 files\n",
      "[0321_201341] Itr 65 processing 15 files\n",
      "[0321_201343] Itr 66 processing 15 files\n",
      "[0321_201345] Itr 67 processing 15 files\n",
      "[0321_201347] Itr 68 processing 15 files\n",
      "[0321_201349] Itr 69 processing 15 files\n",
      "[0321_201351] Itr 70 processing 15 files\n",
      "[0321_201353] Itr 71 processing 15 files\n",
      "[0321_201356] Itr 72 processing 15 files\n",
      "[0321_201358] Itr 73 processing 15 files\n",
      "[0321_201359] Itr 74 processing 15 files\n",
      "[0321_201402] Itr 75 processing 15 files\n",
      "[0321_201404] Itr 76 processing 15 files\n",
      "[0321_201406] Itr 77 processing 15 files\n",
      "[0321_201408] Itr 78 processing 15 files\n",
      "[0321_201409] Itr 79 processing 15 files\n",
      "[0321_201416] Itr 80 processing 15 files\n",
      "[0321_201419] Itr 81 processing 15 files\n",
      "[0321_201421] Itr 82 processing 15 files\n",
      "[0321_201423] Itr 83 processing 15 files\n",
      "[0321_201425] Itr 84 processing 15 files\n",
      "[0321_201427] Itr 85 processing 0 files\n",
      "FILES folder: /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_traces/test\n",
      "in multi_process()\n",
      "Length of FILES: 8\n",
      "[0321_201428] Itr 1 processing 8 files\n",
      "[0321_201429] Itr 2 processing 0 files\n"
     ]
    }
   ],
   "source": [
    "train_hdf_path = os.path.join(data_root, config.modelname, '_hdf', 'train')\n",
    "test1_hdf_path = os.path.join(data_root, config.modelname, '_hdf', 'test1')\n",
    "test2_hdf_path = os.path.join(data_root, config.modelname, '_hdf', 'test2')\n",
    "if os.path.exists(train_features_path) and os.path.exists(test1_hdf_path) and os.path.exists(test2_hdf_path):\n",
    "    print(\"HD5 already exist at {}, {}, and {}\".format(train_features_path, test1_hdf_path, test2_hdf_path))\n",
    "# else:\n",
    "csv_2hdf(\n",
    "    config=config,\n",
    "    data_root=data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27788c-2d8b-4308-8601-a7d1b13df709",
   "metadata": {},
   "source": [
    "### Perform min/max scaling and save scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "577d4aea-c193-42e9-9535-9df0a4d06b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_hdf(file):\n",
    "    with h5py.File(file, 'r') as hdf:\n",
    "        x = hdf['x'][:]\n",
    "        y = hdf['y'][:]\n",
    "    return x, y\n",
    "# feature_columns = ['pkt len (byte)', 'src', 'dst', 'TI0', 'TI1', 'TI2', 'TI3', 'load_dst0_0', 'load_dst1_0', 'load_dst2_0', 'load_dst3_0', 'inter_arr_sys']\n",
    "def min_max(data_root,\n",
    "            feature_columns,\n",
    "            config):\n",
    "    \"\"\"cal. data_range from the training dataset.\"\"\"\n",
    "    src = os.path.join(data_root, '{}/_hdf/train/*.h5'.format(config.modelname))\n",
    "    print(\"path: {}\".format(src))\n",
    "    for i, file in enumerate(glob.glob(src)):\n",
    "        x, y = load_hdf(file)\n",
    "        xmin = pd.Series(x.min(axis=0).min(axis=0), index=feature_columns)\n",
    "        xmax = pd.Series(x.max(axis=0).max(axis=0), index=feature_columns)\n",
    "        ymin = pd.Series(np.array([y.min(axis=0).min(axis=0)]).flatten(),\n",
    "                         index=target)\n",
    "        ymax = pd.Series(np.array([y.max(axis=0).max(axis=0)]).flatten(),\n",
    "                         index=target)\n",
    "        if i == 0:\n",
    "            x_MIN = xmin\n",
    "            x_MAX = xmax\n",
    "            y_MIN = ymin\n",
    "            y_MAX = ymax\n",
    "        else:\n",
    "            x_MIN = np.minimum(x_MIN, xmin)\n",
    "            x_MAX = np.maximum(x_MAX, xmax)\n",
    "            y_MIN = np.minimum(y_MIN, ymin)\n",
    "            y_MAX = np.maximum(y_MAX, ymax)\n",
    "\n",
    "    ins = cScaler(x_MIN, x_MAX, 'x', feature_columns,\n",
    "                  config.no_of_port, config.no_of_buffer)\n",
    "    ins.cluster()\n",
    "    ins.save(os.path.join(data_root, '{}/_scaler'.format(config.modelname)))\n",
    "    cScaler(y_MIN, y_MAX,\n",
    "            'y').save(os.path.join(data_root, '{}/_scaler'.format(config.modelname)))\n",
    "                       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e365902-157f-46f2-b377-4214a731ae3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /scratch/achen/COS561/deepqueuenet/dqn_data/data/4-port switch/FIFO/_hdf/train/*.h5\n"
     ]
    }
   ],
   "source": [
    "scaler_dir = os.path.join(data_root, config.modelname, '_scaler')\n",
    "if os.path.exists(scaler_dir):\n",
    "    shutil.rmtree(scaler_dir)\n",
    "os.makedirs(scaler_dir)\n",
    "feature_columns_save_path = os.path.join(data_root, config.modelname, 'feature_columns.pt')\n",
    "feature_columns = torch.load(feature_columns_save_path)\n",
    "\n",
    "min_max(\n",
    "    data_root=data_root,\n",
    "    feature_columns=feature_columns,\n",
    "    config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed29f84-a4a2-49a0-9e3f-786904e241ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['x', 'y']>\n",
      "x\n",
      "<HDF5 dataset \"x\": shape (11200, 42, 12), type \"<f8\">\n",
      "<HDF5 dataset \"y\": shape (11200, 1), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "hd5_file = 'data/dqn_data/data/4-port switch/FIFO/_hdf/train/4port10link1_0.1_trace1.h5'\n",
    "\n",
    "with h5py.File(hd5_file, \"r\") as f:\n",
    "    # Print all root level object names (aka keys) \n",
    "    # these can be group or dataset names \n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    \n",
    "    a_group_key = list(f.keys())[0]\n",
    "    b_group_key = list(f.keys())[1]\n",
    "    print(a_group_key)\n",
    "    print(f[a_group_key])\n",
    "    print(f[b_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61abe6-1563-4d36-a46b-bdadd0238a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important functions\n",
    "# featureET.model_input() seems to save training data as tfrecords and merge all the train files together, test files together into 1 h5 file\n",
    "# featureET.merge_sample merges reads from merged h5 files\n",
    "# featureET.load_sample loads h5 files\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
